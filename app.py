import streamlit as st
import torch
import os
import pandas as pd
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import shap
import matplotlib.pyplot as plt

# --- 1. é…ç½®èˆ‡è·¯å¾‘è¨­å®š ---
# å‹•æ…‹åµæ¸¬æ¨¡å‹è·¯å¾‘ï¼Œç¢ºä¿åœ¨ä¸åŒé›»è…¦ä¸Šéƒ½èƒ½åŸ·è¡Œ
BASE_DIR = os.path.dirname(__file__)
MODEL_PATH = os.path.join(BASE_DIR, 'best_ai_detector_model')
TEST_DATA_PATH = os.path.join(BASE_DIR, 'test_data.csv')

# æª¢æŸ¥è¨­å‚™ (GPU æˆ– CPU)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 2. æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡å‹è¼‰å…¥ (ä½¿ç”¨å¿«å–) ---
@st.cache_resource
def load_ai_model(path):
    """è¼‰å…¥è¨“ç·´å¥½çš„ Transformers æ¨¡å‹èˆ‡ Tokenizer"""
    if not os.path.exists(path):
        st.error(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹è³‡æ–™å¤¾æ–¼ï¼š{path}\nè«‹ç¢ºèªå·²åŸ·è¡Œè¨“ç·´è…³æœ¬ä¸¦ç”¢ç”Ÿæ¨¡å‹ã€‚")
        st.stop()
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(path)
        model = AutoModelForSequenceClassification.from_pretrained(path)
        model.to(DEVICE)
        model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
        return tokenizer, model
    except Exception as e:
        st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼š{e}")
        st.stop()

@st.cache_data
def load_test_dataset(path):
    """è¼‰å…¥ç”¨æ–¼ Demo é©—è­‰çš„æ¸¬è©¦é›† CSV"""
    if os.path.exists(path):
        return pd.read_csv(path)
    return None

# åˆå§‹åŒ–æ¨¡å‹èˆ‡è³‡æ–™
tokenizer, model = load_ai_model(MODEL_PATH)
test_df = load_test_dataset(TEST_DATA_PATH)

# --- 3. Streamlit ä»‹é¢è¨­è¨ˆ ---
st.set_page_config(page_title="ğŸ¤– AI æ–‡ç« åµæ¸¬å™¨", layout="centered")

st.title("ğŸ¤– AI / Human æ–‡ç« åµæ¸¬å™¨")
st.markdown("""
æœ¬å·¥å…·åˆ©ç”¨ **RoBERTa** æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œåˆ†ææ–‡æœ¬çš„èªç¾©ç‰¹å¾µï¼Œåˆ¤æ–·å…¶æ˜¯ç”±äººå·¥æ’°å¯«é‚„æ˜¯ AI ç”Ÿæˆã€‚
""")

# --- 4. Demo éš¨æ©ŸæŠ½é¸åŠŸèƒ½ ---
st.subheader("ğŸ“ æ­¥é©Ÿä¸€ï¼šè¼¸å…¥æˆ–æŠ½é¸æ–‡æœ¬")

# åˆå§‹åŒ– session_state ä»¥å­˜å„²æŠ½é¸å…§å®¹
if 'input_text' not in st.session_state:
    st.session_state.input_text = ""
if 'actual_label' not in st.session_state:
    st.session_state.actual_label = None

if test_df is not None:
    if st.button("ğŸ² éš¨æ©Ÿå¾æ¸¬è©¦é›† (Test Set) æŠ½é¸ä¸€ç¯‡æ–‡ç« "):
        # éš¨æ©Ÿé¸å–ä¸€åˆ—
        sample = test_df.sample(1).iloc[0]
        st.session_state.input_text = sample['text']
        st.session_state.actual_label = "AI ç”Ÿæˆ" if sample['label'] == 1 else "äººé¡æ’°å¯«"
        st.rerun() # é‡æ–°æ•´ç†ä»¥æ›´æ–°æ–‡å­—æ¡†
else:
    st.info("æç¤ºï¼šè‹¥è¦ä½¿ç”¨éš¨æ©ŸæŠ½é¸åŠŸèƒ½ï¼Œè«‹å…ˆåŸ·è¡Œ split_data.py ç”¢ç”Ÿ test_data.csvã€‚")

# æ–‡æœ¬è¼¸å…¥å€
user_input = st.text_area(
    "è«‹è¼¸å…¥å¾…åˆ†æçš„å…§å®¹ï¼š", 
    value=st.session_state.input_text, 
    height=250, 
    placeholder="åœ¨æ­¤è¼¸å…¥æ–‡ç« æ®µè½..."
)

# è‹¥æœ‰æŠ½é¸å…§å®¹ï¼Œé¡¯ç¤ºçœŸå¯¦æ¨™ç±¤ä»¥ä¾›å°æ¯”
if st.session_state.actual_label:
    st.info(f"ğŸ“ **è³‡æ–™åº«çœŸå¯¦æ¨™ç±¤ï¼š{st.session_state.actual_label}** (åƒ…ä¾› Demo é©—è­‰å°æ¯”)")

# --- 5. åµæ¸¬èˆ‡çµæœé¡¯ç¤º ---
if st.button("ğŸ” åŸ·è¡Œ AI åµæ¸¬", type="primary"):
    if user_input.strip():
        with st.spinner("æ¨¡å‹æ­£åœ¨æ·±åº¦åˆ†æä¸­ï¼Œè«‹ç¨å€™..."):
            # 1. é è™•ç†æ–‡æœ¬
            inputs = tokenizer(
                user_input, 
                return_tensors="pt", 
                truncation=True, 
                padding=True, 
                max_length=512
            ).to(DEVICE)
            
            # 2. æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = model(**inputs)
                # ä½¿ç”¨ Softmax è½‰åŒ–ç‚ºæ¦‚ç‡
                probs = F.softmax(outputs.logits, dim=-1).squeeze().tolist()
            
            human_prob = probs[0]
            ai_prob = probs[1]

            # 3. çµ±è¨ˆé‡èˆ‡å¯è¦–åŒ–å±•ç¤º
            st.divider()
            st.subheader("ğŸ“Š æ–‡æœ¬ç‰¹å¾µçµ±è¨ˆèˆ‡é æ¸¬")
            
            # ç¬¬ä¸€å±¤ï¼šæ ¸å¿ƒæŒ‡æ¨™
            col1, col2, col3 = st.columns(3)
            col1.metric("ğŸ§‘ğŸ» äººé¡æ¦‚ç‡", f"{human_prob:.2%}")
            col2.metric("ğŸ¤– AI æ¦‚ç‡", f"{ai_prob:.2%}")
            col3.metric("ğŸ“ å­—æ•¸çµ±è¨ˆ", len(user_input.split()))

            # ç¬¬äºŒå±¤ï¼šæ¦‚ç‡åˆ†ä½ˆæ¢
            st.write("**æ¨¡å‹ä¿¡å¿ƒåˆ†ä½ˆåœ–ï¼š**")
            chart_data = pd.DataFrame({
                "ä¾†æº": ["äººé¡ (Human)", "äººå·¥æ™ºæ…§ (AI)"],
                "æ©Ÿç‡ (%)": [human_prob * 100, ai_prob * 100]
            })
            st.bar_chart(chart_data.set_index("ä¾†æº"))

            # ç¬¬ä¸‰å±¤ï¼šçµè«–å ±å‘Š
            if ai_prob > 0.5:
                st.warning(f"ğŸš¨ **åˆ¤å®šçµè«–ï¼šé«˜åº¦ç–‘ä¼¼ç‚º AI ç”Ÿæˆå…§å®¹**")
                st.info(f"æ¨¡å‹åˆ†æé¡¯ç¤ºï¼Œè©²æ–‡æœ¬å…·æœ‰æ˜é¡¯çš„èªè¨€æ¨¡å‹ç‰¹å¾µï¼ŒAI ä¿¡å¿ƒåº¦ç‚º {ai_prob:.1%}")
            else:
                st.success(f"âœ… **åˆ¤å®šçµè«–ï¼šé«˜åº¦ç–‘ä¼¼ç‚ºäººé¡æ’°å¯«å…§å®¹**")
                st.info(f"æ¨¡å‹åˆ†æé¡¯ç¤ºï¼Œè©²æ–‡æœ¬èªç¾©æµå‹•è¼ƒç¬¦åˆäººé¡ç¿’æ…£ï¼Œäººé¡ä¿¡å¿ƒåº¦ç‚º {human_prob:.1%}")

            # è‹¥æœ‰æŠ½é¸è³‡æ–™ï¼Œé¡¯ç¤ºæ¨™ç±¤å°æ¯” (å¢åŠ çµ±è¨ˆå¯ä¿¡åº¦)
            if st.session_state.actual_label:
                st.markdown(f"**é©—è­‰å°æ¯”ï¼š** çœŸå¯¦æ¨™ç±¤ç‚º `{st.session_state.actual_label}`")
    else:
        st.error("è«‹è¼¸å…¥å…§å®¹å¾Œå†åŸ·è¡Œåµæ¸¬ï¼")

# --- 6. é€²éšåˆ†æï¼šSHAP è§£é‡‹å™¨ ---
st.subheader("ğŸ” é€²éšç‰¹å¾µåˆ†æ (å¯è§£é‡‹ AI)")
if st.button("ğŸ§¬ åŸ·è¡Œ SHAP é—œéµå­—åˆ†æ"):
    if user_input.strip():
        with st.spinner("æ­£åœ¨è¨ˆç®—å–®è©è²¢ç»åº¦ï¼Œé€™å¯èƒ½éœ€è¦å¹¾åç§’..."):
            try:
                # ä¿®æ­£å¾Œçš„é æ¸¬å‡½æ•¸
                def predict_probs(texts):
                    # SHAP æœ‰æ™‚æœƒå‚³å…¥ numpy é™£åˆ—ï¼Œéœ€ç¢ºä¿è½‰ç‚º list
                    texts = [str(t) for t in texts] 
                    inputs = tokenizer(
                        texts, 
                        return_tensors="pt", 
                        padding=True, 
                        truncation=True, 
                        max_length=512
                    ).to(DEVICE)
                    
                    with torch.no_grad():
                        logits = model(**inputs).logits
                        # SHAP éœ€è¦çš„æ˜¯æ©Ÿç‡å€¼ (Probability)
                        probs = torch.softmax(logits, dim=-1).cpu().numpy()
                    return probs

                # ä½¿ç”¨ shap.maskers.Text è™•ç† Tokenizer ç¢ºä¿å°é½Š
                masker = shap.maskers.Text(tokenizer)
                
                # åˆå§‹åŒ–è§£é‡‹å™¨
                explainer = shap.Explainer(predict_probs, masker=masker, output_names=["Human", "AI"])
                
                # è¨ˆç®— SHAP å€¼
                shap_values = explainer([user_input])

                # è¦–è¦ºåŒ–è¼¸å‡º
                st.write("**é—œéµè©å½±éŸ¿åŠ›åˆ†æï¼š**")
                st.caption("ğŸ”´ ç´…è‰²ï¼šå¢åŠ  AI ç”Ÿæˆç–‘æ…® | ğŸ”µ è—è‰²ï¼šåå‘äººé¡æ’°å¯«ç‰¹å¾µ")
                
                # ä½¿ç”¨ HTML æ¸²æŸ“ SHAP çµæœ
                shap_html = shap.plots.text(shap_values[0], display=False)
                st.components.v1.html(shap_html, height=400, scrolling=True)
                
            except Exception as e:
                st.error(f"SHAP åˆ†æç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
                st.info("æç¤ºï¼šé€™å¯èƒ½æ˜¯å› ç‚ºæ–‡æœ¬éçŸ­æˆ–åŒ…å«ç‰¹æ®Šå­—å…ƒï¼Œè«‹å˜—è©¦æ›´æ›ä¸€æ®µæ–‡å­—ã€‚")
    else:
        st.error("è«‹å…ˆè¼¸å…¥æ–‡å­—å†åŸ·è¡Œåˆ†æï¼")

# é å°¾èªªæ˜
st.divider()
st.caption("æŠ€è¡“åº•å±¤ï¼šTransformers RoBERTa-base | æ•¸æ“šé›†ï¼štrain_v2_drcat_02")